{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.layers import Input ,Embedding ,Dense ,Concatenate ,LSTM, Bidirectional , TimeDistributed \n",
    "from tensorflow.keras.models import Model , Sequential\n",
    "\n",
    "import warnings\n",
    "pd.set_option('display.max_colwidth',200)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Reviews.csv\",nrows = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'],inplace =True)\n",
    "data.dropna(axis=0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =set(stopwords.words('english'))\n",
    "def Textcleaner(text):\n",
    "    string = text.lower()\n",
    "    string = BeautifulSoup(string,\"lxml\").text\n",
    "    string = re.sub(r'\\([^)]*\\)', '', string)\n",
    "    string = re.sub('\"','', string)\n",
    "    string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in string.split(\" \")])    \n",
    "    string = re.sub(r\"'s\\b\",\"\",string)\n",
    "    string = re.sub(\"[^a-zA-Z]\", \" \", string)\n",
    "    tokens = [w for w in string.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "cleaned_text =[]\n",
    "for i in data['Text']:\n",
    "    cleaned_text.append(Textcleaner(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Summarycleaner(text):\n",
    "    string = re.sub('\"','', text)\n",
    "    string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in string.split(\" \")])    \n",
    "    string = re.sub(r\"'s\\b\",\"\",string)\n",
    "    string = re.sub(\"[^a-zA-Z]\", \" \", string)\n",
    "    string = string.lower()\n",
    "    tokens = string.split()\n",
    "    string=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            string=string+i+' '  \n",
    "    return string\n",
    "\n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(Summarycleaner(t))\n",
    "\n",
    "data['cleaned_text'] = cleaned_text\n",
    "data['cleaned_summary'] = cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Summary: good quality dog food \n",
      "\n",
      "\n",
      "Review: product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Summary: not as advertised \n",
      "\n",
      "\n",
      "Review: confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "Summary: delight says it all \n",
      "\n",
      "\n",
      "Review: looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal\n",
      "Summary: cough medicine \n",
      "\n",
      "\n",
      "Review: great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "Summary: great taffy \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review:\",data['cleaned_text'][i])\n",
    "    print(\"Summary:\",data['cleaned_summary'][i])\n",
    "    print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot graph to analyse maximum text and summary length\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_count = []\n",
    "summary_count = []\n",
    "\n",
    "for i in data['cleaned_text']:\n",
    "    text_count.append(len(i.split()))\n",
    "    \n",
    "for i in data['cleaned_summary']:\n",
    "    summary_count.append(len(i.split()))\n",
    "    \n",
    "length = pd.DataFrame({'text': text_count,'summary':summary_count})\n",
    "length.hist(bins=30)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len = 50\n",
    "max_summary_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x :'sos ' + x + ' eos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                sos good quality dog food  eos\n",
       "1                                    sos not as advertised  eos\n",
       "2                                  sos delight says it all  eos\n",
       "3                                       sos cough medicine  eos\n",
       "4                                          sos great taffy  eos\n",
       "                                 ...                           \n",
       "3997                              sos best for oyster soup  eos\n",
       "3998                                           sos amazing  eos\n",
       "3999                sos the cavemen must have been wealthy  eos\n",
       "4000                           sos okay in pinch not great  eos\n",
       "4001    sos they are good except for the rainforest flavor  eos\n",
       "Name: summary, Length: 4002, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data in training and validation sets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_val , y_train , y_val = train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(x_train)\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =   x_tokenizer.texts_to_sequences(x_train) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "x_train =   pad_sequences(x_train,maxlen=max_text_len, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_text_len, padding='post')\n",
    "\n",
    "x_voc_size   =  len(x_tokenizer.word_index)+1                          #NOT SURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tokenizer = Tokenizer()\n",
    "y_train = list(y_train)\n",
    "y_tokenizer.fit_on_texts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train =   y_tokenizer.texts_to_sequences(y_train) \n",
    "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "y_train =   pad_sequences(y_train, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "y_voc_size  =  len(y_tokenizer.word_index) +1                          #NOT SURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3601, 3601)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sos'],len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows contaning only START and END tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = []\n",
    "for i in range(len(y_train)):\n",
    "    cnt=0\n",
    "    for j in y_train[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_train=np.delete(y_train,ind, axis=0)\n",
    "x_train=np.delete(x_train,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 500\n",
    "embedding_dim = 500\n",
    "\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "enc_emb =  Embedding(x_voc_size, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4))\n",
    "encoder_output1, forward_h1, forward_c1 ,backward_h1,backward_c1= encoder_lstm1(enc_emb)\n",
    "state_h1 = Concatenate()([forward_h1, backward_h1])\n",
    "state_c1 = Concatenate()([forward_c1, backward_c1])\n",
    "#encoder_states = [state_h1, state_c1]\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4))\n",
    "encoder_output2, forward_h2, forward_c2 ,backward_h2,backward_c2= encoder_lstm2(encoder_output1)\n",
    "state_h2 = Concatenate()([forward_h2, backward_h2])\n",
    "state_c2 = Concatenate()([forward_c2, backward_c2])\n",
    "#encoder_states = [state_h2, state_c2]\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4))\n",
    "encoder_output3, forward_h3, forward_c3 ,backward_h3,backward_c3= encoder_lstm3(encoder_output2)\n",
    "state_h3 = Concatenate()([forward_h3, backward_h3])\n",
    "state_c3 = Concatenate()([forward_c3, backward_c3])\n",
    "encoder_states = [state_h3, state_c3]\n",
    "\n",
    "\n",
    "#decoder layer\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dec_emb = Embedding(y_voc_size, embedding_dim,trainable=True)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(2*latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_h , decoder_c= decoder_lstm(dec_emb,initial_state=encoder_states)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 500)      4179500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 50, 1000), ( 4004000     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 50, 1000), ( 6004000     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 50, 1000), ( 6004000     bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 500)    1170500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1000)         0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000)         0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 1000), 6004000     embedding_1[0][0]                \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer [(None, None, 1000), 2001000     bidirectional_2[0][0]            \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 2000)   0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 2341)   4684341     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 34,051,341\n",
      "Trainable params: 34,051,341\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#using customised attention layer\n",
    "import attention as at\n",
    "attn_layer = at.AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_output3, decoder_outputs])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3601 samples, validate on 393 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Shruti\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "#training model\n",
    "history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:] ,epochs=25,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9327fb3ba6b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreverse_target_word_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mreverse_source_word_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtarget_word_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "dec_emb2= Embedding(y_voc_size, embedding_dim,trainable=True)(decoder_inputs) \n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],[decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq):\n",
    "    e_out ,e_h ,e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    target_seq[0,0] = target_word_index['sos']\n",
    "    \n",
    "    stop_condition = False\n",
    "    \n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition :\n",
    "        \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!=' eos'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "            \n",
    "        if(sampled_token!=' eos' or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "            \n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        e_h, e_c = h, c\n",
    "        \n",
    "        \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert encoded seq to summary and text\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sos']) and i!=target_word_index['eos']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: two english terrier bad allergies one picky quickly stopped eating type dog food bought eating newman advanced dog formula active senior dogs months tired skin improved love buy subscription never worry running cheaper way purchase local stores sell small bags expensive way buy \n",
      "Original summary: great for sensitive dogs \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: tea drinking honestly say best earl grey ever tasted ordered number times batch consistently excellent even given gift container superb keeping loose tea fresh metal canister lid knob tea stays dry fresh length time takes use favor like earl grey treat tea \n",
      "Original summary: favorite tea \n",
      "Predicted summary:  best\n",
      "\n",
      "\n",
      "Review: soup rather meal outside little taste little chicken mostly rice weak chicken stock potato corn bad want light convenient snack chef meals tastier also convenient pull lid comes away easily hormel compleats top first afterwards easy especially hot \n",
      "Original summary: not lot of flavor not lot of chicken \n",
      "Predicted summary:  the\n",
      "\n",
      "\n",
      "Review: purchased mix use new presto belgian waffle maker pleased results took cups mix make large belgian waffles cooked minutes flavor mix slightly sweet aftertaste like get cheap commercial brands like bisquick buying sure \n",
      "Original summary: great waffle \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: nervous reading reviews taste quality consuming couple days really enjoy thought would gross really nice fruit kind taste great shipping great quality amazing def \n",
      "Original summary: better than expected \n",
      "Predicted summary:  great\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the performance of the model\n",
    "for i in range(0,5):\n",
    "    print(\"Review:\",seq2text(x_train[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_train[i]))\n",
    "    print(\"Predicted summary:\",decode_seq(x_train[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
