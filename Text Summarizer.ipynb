{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.layers import Input ,Embedding ,Dense ,Concatenate ,LSTM, Bidirectional , TimeDistributed \n",
    "from tensorflow.keras.models import Model , Sequential\n",
    "\n",
    "import warnings\n",
    "pd.set_option('display.max_colwidth',200)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Reviews.csv\",nrows = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'],inplace =True)\n",
    "data.dropna(axis=0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =set(stopwords.words('english'))\n",
    "def Textcleaner(text):\n",
    "    string = text.lower()\n",
    "    string = BeautifulSoup(string,\"lxml\").text\n",
    "    string = re.sub(r'\\([^)]*\\)', '', string)\n",
    "    string = re.sub('\"','', string)\n",
    "    string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in string.split(\" \")])    \n",
    "    string = re.sub(r\"'s\\b\",\"\",string)\n",
    "    string = re.sub(\"[^a-zA-Z]\", \" \", string)\n",
    "    tokens = [w for w in string.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "cleaned_text =[]\n",
    "for i in data['Text']:\n",
    "    cleaned_text.append(Textcleaner(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Summarycleaner(text):\n",
    "    string = re.sub('\"','', text)\n",
    "    string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in string.split(\" \")])    \n",
    "    string = re.sub(r\"'s\\b\",\"\",string)\n",
    "    string = re.sub(\"[^a-zA-Z]\", \" \", string)\n",
    "    string = string.lower()\n",
    "    tokens = string.split()\n",
    "    string=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            string=string+i+' '  \n",
    "    return string\n",
    "\n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(Summarycleaner(t))\n",
    "\n",
    "data['cleaned_text'] = cleaned_text\n",
    "data['cleaned_summary'] = cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Summary: good quality dog food \n",
      "\n",
      "\n",
      "Review: product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Summary: not as advertised \n",
      "\n",
      "\n",
      "Review: confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "Summary: delight says it all \n",
      "\n",
      "\n",
      "Review: looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal\n",
      "Summary: cough medicine \n",
      "\n",
      "\n",
      "Review: great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "Summary: great taffy \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review:\",data['cleaned_text'][i])\n",
    "    print(\"Summary:\",data['cleaned_summary'][i])\n",
    "    print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_count = []\n",
    "summary_count = []\n",
    "\n",
    "for i in data['cleaned_text']:\n",
    "    text_count.append(len(i.split()))\n",
    "    \n",
    "for i in data['cleaned_summary']:\n",
    "    summary_count.append(len(i.split()))\n",
    "    \n",
    "length = pd.DataFrame({'text': text_count,'summary':summary_count})\n",
    "length.hist(bins=30)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len = 50\n",
    "max_summary_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x :'sos ' + x + ' eos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                sos good quality dog food  eos\n",
       "1                                    sos not as advertised  eos\n",
       "2                                  sos delight says it all  eos\n",
       "3                                       sos cough medicine  eos\n",
       "4                                          sos great taffy  eos\n",
       "                                 ...                           \n",
       "3997                              sos best for oyster soup  eos\n",
       "3998                                           sos amazing  eos\n",
       "3999                sos the cavemen must have been wealthy  eos\n",
       "4000                           sos okay in pinch not great  eos\n",
       "4001    sos they are good except for the rainforest flavor  eos\n",
       "Name: summary, Length: 4002, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_val , y_train , y_val = train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(x_train)\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 68.21009810959559\n",
      "Total Coverage of rare words: 9.499376279803235\n"
     ]
    }
   ],
   "source": [
    "thresh = 4\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "freq = 0\n",
    "tot_freq = 0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt  = tot_cnt+1\n",
    "    tot_freq = tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt  = cnt+1\n",
    "        freq = freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =   x_tokenizer.texts_to_sequences(x_train) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "x_train =   pad_sequences(x_train,maxlen=max_text_len, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_text_len, padding='post')\n",
    "\n",
    "x_voc_size   =  x_tokenizer.num_words +1                          #NOT SURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tokenizer = Tokenizer()\n",
    "y_train = list(y_train)\n",
    "y_tokenizer.fit_on_texts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 85.2991452991453\n",
      "Total Coverage of rare words: 16.730440036250126\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "y_train =   y_tokenizer.texts_to_sequences(y_train) \n",
    "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "y_train =   pad_sequences(y_train, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "y_voc_size  =  y_tokenizer.num_words +1                          #NOT SURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3601, 3601)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sos'],len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows contaning only START and END tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = []\n",
    "for i in range(len(y_train)):\n",
    "    cnt=0\n",
    "    for j in y_train[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_train,ind, axis=0)\n",
    "x_tr=np.delete(x_train,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shruti\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Shruti\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim = 100\n",
    "\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "enc_emb =  Embedding(x_voc_size, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "encoder_states = [state_h,state_c]\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dec_emb = Embedding(y_voc_size, embedding_dim,trainable=True)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 100)      265800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 300), (N 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 300), (N 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    34500       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 300), (N 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer [(None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 345)    207345      concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,092,745\n",
      "Trainable params: 3,092,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import attention as at\n",
    "attn_layer = at.AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3601 samples, validate on 369 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Shruti\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "3601/3601 [==============================] - 159s 44ms/sample - loss: 2.7888 - acc: 0.5875 - val_loss: 1.9337 - val_acc: 0.6218\n",
      "Epoch 2/25\n",
      "3601/3601 [==============================] - 164s 46ms/sample - loss: 1.9195 - acc: 0.6522 - val_loss: 1.7880 - val_acc: 0.6841\n",
      "Epoch 3/25\n",
      "3601/3601 [==============================] - 174s 48ms/sample - loss: 1.7313 - acc: 0.7040 - val_loss: 1.6635 - val_acc: 0.7212\n",
      "Epoch 4/25\n",
      "3601/3601 [==============================] - 182s 51ms/sample - loss: 1.6545 - acc: 0.7167 - val_loss: 1.6370 - val_acc: 0.7221\n",
      "Epoch 5/25\n",
      "3601/3601 [==============================] - 189s 52ms/sample - loss: 1.6324 - acc: 0.7167 - val_loss: 1.6203 - val_acc: 0.7221\n",
      "Epoch 6/25\n",
      "3601/3601 [==============================] - 190s 53ms/sample - loss: 1.6086 - acc: 0.7163 - val_loss: 1.6062 - val_acc: 0.7224\n",
      "Epoch 7/25\n",
      "3601/3601 [==============================] - 199s 55ms/sample - loss: 1.5864 - acc: 0.7168 - val_loss: 1.6096 - val_acc: 0.7221\n",
      "Epoch 8/25\n",
      "3601/3601 [==============================] - 192s 53ms/sample - loss: 1.5772 - acc: 0.7169 - val_loss: 1.5926 - val_acc: 0.7233\n",
      "Epoch 9/25\n",
      "3601/3601 [==============================] - 198s 55ms/sample - loss: 1.5522 - acc: 0.7185 - val_loss: 1.5765 - val_acc: 0.7245\n",
      "Epoch 10/25\n",
      "3601/3601 [==============================] - 207s 57ms/sample - loss: 1.5308 - acc: 0.7199 - val_loss: 1.5799 - val_acc: 0.7197\n",
      "Epoch 11/25\n",
      "3601/3601 [==============================] - 209s 58ms/sample - loss: 1.5184 - acc: 0.7192 - val_loss: 1.5698 - val_acc: 0.7221\n",
      "Epoch 12/25\n",
      "3601/3601 [==============================] - 210s 58ms/sample - loss: 1.5076 - acc: 0.7205 - val_loss: 1.5560 - val_acc: 0.7227\n",
      "Epoch 13/25\n",
      "3601/3601 [==============================] - 179s 50ms/sample - loss: 1.4905 - acc: 0.7216 - val_loss: 1.5455 - val_acc: 0.7224\n",
      "Epoch 14/25\n",
      "3601/3601 [==============================] - 144s 40ms/sample - loss: 1.4778 - acc: 0.7220 - val_loss: 1.5431 - val_acc: 0.7248\n",
      "Epoch 15/25\n",
      "3601/3601 [==============================] - 141s 39ms/sample - loss: 1.4626 - acc: 0.7242 - val_loss: 1.5289 - val_acc: 0.7242\n",
      "Epoch 16/25\n",
      "3601/3601 [==============================] - 146s 40ms/sample - loss: 1.4586 - acc: 0.7257 - val_loss: 1.5277 - val_acc: 0.7239\n",
      "Epoch 17/25\n",
      "3601/3601 [==============================] - 140s 39ms/sample - loss: 1.4524 - acc: 0.7262 - val_loss: 1.5115 - val_acc: 0.7290\n",
      "Epoch 18/25\n",
      "3601/3601 [==============================] - 147s 41ms/sample - loss: 1.4354 - acc: 0.7278 - val_loss: 1.5035 - val_acc: 0.7266\n",
      "Epoch 19/25\n",
      "3601/3601 [==============================] - 141s 39ms/sample - loss: 1.4229 - acc: 0.7278 - val_loss: 1.4964 - val_acc: 0.7257\n",
      "Epoch 20/25\n",
      "3601/3601 [==============================] - 139s 39ms/sample - loss: 1.4110 - acc: 0.7291 - val_loss: 1.4915 - val_acc: 0.7311\n",
      "Epoch 21/25\n",
      "3601/3601 [==============================] - 137s 38ms/sample - loss: 1.3954 - acc: 0.7296 - val_loss: 1.4870 - val_acc: 0.7323\n",
      "Epoch 22/25\n",
      "3601/3601 [==============================] - 142s 39ms/sample - loss: 1.3838 - acc: 0.7309 - val_loss: 1.4885 - val_acc: 0.7320\n",
      "Epoch 23/25\n",
      "3601/3601 [==============================] - 143s 40ms/sample - loss: 1.3752 - acc: 0.7321 - val_loss: 1.4763 - val_acc: 0.7308\n",
      "Epoch 24/25\n",
      "3601/3601 [==============================] - 138s 38ms/sample - loss: 1.3605 - acc: 0.7328 - val_loss: 1.4859 - val_acc: 0.7263\n",
      "Epoch 25/25\n",
      "3601/3601 [==============================] - 140s 39ms/sample - loss: 1.3494 - acc: 0.7329 - val_loss: 1.4800 - val_acc: 0.7302\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:] ,epochs=25,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'new_model(1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-36286dcc7db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"new_model(1).h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    221\u001b[0m   \u001b[0mopened_new_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'new_model(1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "mod = load_model(\"new_model(1).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Unrecognized keyword arguments:', dict_keys(['ragged']))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0fcb424c51ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_config.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mjson_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnew_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mmodel_from_json\u001b[1;34m(json_string, custom_objects)\u001b[0m\n\u001b[0;32m    363\u001b[0m   \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    190\u001b[0m             custom_objects=dict(\n\u001b[0;32m    191\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                 list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    193\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m       \u001b[0mprocess_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m     \u001b[1;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m   1247\u001b[0m       \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m       \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    192\u001b[0m                 list(custom_objects.items())))\n\u001b[0;32m    193\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m       \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \"\"\"\n\u001b[1;32m--> 402\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m       \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Unrecognized keyword arguments:', dict_keys(['ragged']))"
     ]
    }
   ],
   "source": [
    "with open('model_config.json') as json_file:\n",
    "    json_config = json_file.read()\n",
    "new_model = tf.keras.models.model_from_json(json_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown mat file type, version 0, 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7b0344524810>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mMR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[1;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m     71\u001b[0m     \u001b[0mbyte_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmjv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mMatFile4Reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_opened\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\miobase.py\u001b[0m in \u001b[0;36mget_matfile_version\u001b[1;34m(fileobj)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaj_val\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown mat file type, version %s, %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown mat file type, version 0, 0"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "f = scipy.io.loadmat('new_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9327fb3ba6b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreverse_target_word_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mreverse_source_word_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtarget_word_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "dec_emb2= Embedding(y_voc_size, embedding_dim,trainable=True)(decoder_inputs) \n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],[decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq):\n",
    "    e_out ,e_h ,e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    target_seq[0,0] = target_word_index['sos']\n",
    "    \n",
    "    stop_condition = False\n",
    "    \n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition :\n",
    "        \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!=' eos'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "            \n",
    "        if(sampled_token!=' eos' or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "            \n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        e_h, e_c = h, c\n",
    "        \n",
    "        \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sos']) and i!=target_word_index['eos']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: two english terrier bad allergies one picky quickly stopped eating type dog food bought eating newman advanced dog formula active senior dogs months tired skin improved love buy subscription never worry running cheaper way purchase local stores sell small bags expensive way buy \n",
      "Original summary: great for sensitive dogs \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: tea drinking honestly say best earl grey ever tasted ordered number times batch consistently excellent even given gift container superb keeping loose tea fresh metal canister lid knob tea stays dry fresh length time takes use favor like earl grey treat tea \n",
      "Original summary: favorite tea \n",
      "Predicted summary:  best\n",
      "\n",
      "\n",
      "Review: soup rather meal outside little taste little chicken mostly rice weak chicken stock potato corn bad want light convenient snack chef meals tastier also convenient pull lid comes away easily hormel compleats top first afterwards easy especially hot \n",
      "Original summary: not lot of flavor not lot of chicken \n",
      "Predicted summary:  the\n",
      "\n",
      "\n",
      "Review: purchased mix use new presto belgian waffle maker pleased results took cups mix make large belgian waffles cooked minutes flavor mix slightly sweet aftertaste like get cheap commercial brands like bisquick buying sure \n",
      "Original summary: great waffle \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: nervous reading reviews taste quality consuming couple days really enjoy thought would gross really nice fruit kind taste great shipping great quality amazing def \n",
      "Original summary: better than expected \n",
      "Predicted summary:  great\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"Review:\",seq2text(x_train[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_train[i]))\n",
    "    print(\"Predicted summary:\",decode_seq(x_train[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-b664f90957ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget_word_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'sos'"
     ]
    }
   ],
   "source": [
    "target_word_index['sos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Unrecognized keyword arguments:', dict_keys(['ragged']))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-4b09befce2b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    232\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;31m# set weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    322\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[0;32m    323\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    190\u001b[0m             custom_objects=dict(\n\u001b[0;32m    191\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                 list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    193\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m       \u001b[0mprocess_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m     \u001b[1;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m   1247\u001b[0m       \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m       \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    192\u001b[0m                 list(custom_objects.items())))\n\u001b[0;32m    193\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m       \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \"\"\"\n\u001b[1;32m--> 402\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized keyword arguments:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Unrecognized keyword arguments:', dict_keys(['ragged']))"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "mod = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
